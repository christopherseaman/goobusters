{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS_TOKEN=d2b086facd41171613d918a9abefe499\n",
      "DATA_DIR=data\n",
      "DOMAIN=ucsf.md.ai\n",
      "PROJECT_ID=x9N2LJBZ\n",
      "DATASET_ID=D_V688LQ\n",
      "ANNOTATIONS=data/mdai_ucsf_project_x9N2LJBZ_annotations_2024-06-27-212520.json\n",
      "LABEL_ID=L_13yPql\n",
      "Successfully authenticated to ucsf.md.ai.\n",
      "Using path 'data' for data.\n",
      "Preparing annotations export for project x9N2LJBZ...                                                \n",
      "Using cached annotations data for project x9N2LJBZ.\n",
      "Preparing images export for project x9N2LJBZ...                                                     \n",
      "Using cached images data for project x9N2LJBZ.\n",
      "Annotations with corresponding video files: 226\n",
      "Annotations without corresponding video files: 5\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set constants\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mdai\n",
    "from mdai.visualize import load_mask, display_images\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "load_dotenv('dot.env')\n",
    "\n",
    "ACCESS_TOKEN = os.getenv('MDAI_TOKEN')\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "DOMAIN = os.getenv('DOMAIN')\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "DATASET_ID = os.getenv('DATASET_ID')\n",
    "ANNOTATIONS = os.path.join(DATA_DIR, os.getenv('ANNOTATIONS'))\n",
    "LABEL_ID = os.getenv('LABEL_ID')\n",
    "\n",
    "# Define an error threshold to filter out low-confidence points\n",
    "ERROR_THRESHOLD = 1.0\n",
    "\n",
    "print(f\"ACCESS_TOKEN={ACCESS_TOKEN}\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"DOMAIN={DOMAIN}\")\n",
    "print(f\"PROJECT_ID={PROJECT_ID}\")\n",
    "print(f\"DATASET_ID={DATASET_ID}\")\n",
    "print(f\"ANNOTATIONS={ANNOTATIONS}\")\n",
    "print(f\"LABEL_ID={LABEL_ID}\")\n",
    "\n",
    "# Start MD.ai client\n",
    "mdai_client = mdai.Client(domain=DOMAIN, access_token=ACCESS_TOKEN)\n",
    "\n",
    "# Download the dataset from MD.ai (or use cached version)\n",
    "project = mdai_client.project(project_id=PROJECT_ID, path=DATA_DIR)\n",
    "\n",
    "# Load the annotations\n",
    "annotations_data = mdai.common_utils.json_to_dataframe(ANNOTATIONS)\n",
    "annotations_df = pd.DataFrame(annotations_data['annotations'])\n",
    "labels = annotations_df['labelId'].unique()\n",
    "\n",
    "# Create the label map, LABEL_ID => 1, others in labels => 0\n",
    "labels_dict = {LABEL_ID: 1}\n",
    "project.set_labels_dict(labels_dict)\n",
    "\n",
    "# Get the dataset\n",
    "dataset = project.get_dataset_by_id(DATASET_ID)\n",
    "dataset.classes_dict = project.classes_dict \n",
    "\n",
    "# Ensure BASE is set after preparing the dataset\n",
    "BASE = dataset.images_dir\n",
    "\n",
    "# Filter annotations for the free fluid label\n",
    "free_fluid_annotations = annotations_df[annotations_df['labelId'] == LABEL_ID].copy()\n",
    "\n",
    "# Function to construct the video path\n",
    "def construct_video_path(base_dir, study_uid, series_uid):\n",
    "    return os.path.join(base_dir, study_uid, f\"{series_uid}.mp4\")\n",
    "\n",
    "# Add video paths to the dataframe using .loc to avoid the SettingWithCopyWarning\n",
    "free_fluid_annotations['video_path'] = free_fluid_annotations.apply(\n",
    "    lambda row: construct_video_path(BASE, row['StudyInstanceUID'], row['SeriesInstanceUID']), axis=1)\n",
    "\n",
    "# Check if video files exist and add the result to the dataframe using .loc\n",
    "free_fluid_annotations['file_exists'] = free_fluid_annotations['video_path'].apply(os.path.exists)\n",
    "\n",
    "# Count the number of annotations with and without corresponding video files\n",
    "num_with_files = free_fluid_annotations['file_exists'].sum()\n",
    "num_without_files = len(free_fluid_annotations) - num_with_files\n",
    "\n",
    "print(f\"Annotations with corresponding video files: {num_with_files}\")\n",
    "print(f\"Annotations without corresponding video files: {num_without_files}\")\n",
    "\n",
    "# Select five random annotations with corresponding video files\n",
    "if DEBUG:\n",
    "    random_annotations = free_fluid_annotations[free_fluid_annotations['file_exists']].sample(n=1, random_state=42)\n",
    "else:\n",
    "    random_annotations = free_fluid_annotations[free_fluid_annotations['file_exists']].sample(n=5, random_state=42)\n",
    "\n",
    "# Display function\n",
    "def polygons_to_mask(polygons, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    for polygon in polygons:\n",
    "        points = np.array(polygon, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "def display_annotation(row):\n",
    "    video_path = row['video_path']\n",
    "    frame_number = int(row['frameNumber'])\n",
    "    foreground = row['data']['foreground']\n",
    "    video_id = row['SeriesInstanceUID']\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "\n",
    "    mask = polygons_to_mask(foreground, frame.shape[0], frame.shape[1])\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame[mask == 1] = (0, 0, 255)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    ax[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    ax[0].set_title(f'Video ID: {video_id}')\n",
    "    ax[1].imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
    "    # ax[1].set_title(f'Annotated Frame (Video ID: {video_id}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video: 1.2.826.0.1.3680043.8.498.19685680876768794543773205326660692566; Frame: 15...\n",
      "Initial mask shape: (540, 720)\n",
      "Initial mask min: 0, max: 1\n",
      "Frame 15: mask min: 0.0, max: 1.0\n",
      "Frame 14: mask min: 0.0, max: 0.0\n",
      "Frame 13: mask min: 0.0, max: 0.0\n",
      "Frame 12: mask min: 0.0, max: 0.0\n",
      "Frame 11: mask min: 0.0, max: 0.0\n",
      "Frame 10: mask min: 0.0, max: 0.0\n",
      "Frame 9: mask min: 0.0, max: 0.0\n",
      "Frame 8: mask min: 0.0, max: 0.0\n",
      "Frame 7: mask min: 0.0, max: 0.0\n",
      "Frame 6: mask min: 0.0, max: 0.0\n",
      "Frame 5: mask min: 0.0, max: 0.0\n",
      "Frame 4: mask min: 0.0, max: 0.0\n",
      "Frame 3: mask min: 0.0, max: 0.0\n",
      "Frame 2: mask min: 0.0, max: 0.0\n",
      "Frame 1: mask min: 0.0, max: 0.0\n",
      "Frame 0: mask min: 0.0, max: 0.0\n",
      "Frame 15: mask min: 0.0, max: 1.0\n",
      "Frame 16: mask min: 0.0, max: 0.0\n",
      "Frame 17: mask min: 0.0, max: 0.0\n",
      "Frame 18: mask min: 0.0, max: 0.0\n",
      "Frame 19: mask min: 0.0, max: 0.0\n",
      "Frame 20: mask min: 0.0, max: 0.0\n",
      "Frame 21: mask min: 0.0, max: 0.0\n",
      "Frame 22: mask min: 0.0, max: 0.0\n",
      "Frame 23: mask min: 0.0, max: 0.0\n",
      "Frame 24: mask min: 0.0, max: 0.0\n",
      "Frame 25: mask min: 0.0, max: 0.0\n",
      "Frame 26: mask min: 0.0, max: 0.0\n",
      "Frame 27: mask min: 0.0, max: 0.0\n",
      "Frame 28: mask min: 0.0, max: 0.0\n",
      "Frame 29: mask min: 0.0, max: 0.0\n",
      "Frame 30: mask min: 0.0, max: 0.0\n",
      "Frame 31: mask min: 0.0, max: 0.0\n",
      "Frame 32: mask min: 0.0, max: 0.0\n",
      "Frame 33: mask min: 0.0, max: 0.0\n",
      "Frame 34: mask min: 0.0, max: 0.0\n",
      "Frame 35: mask min: 0.0, max: 0.0\n",
      "Frame 36: mask min: 0.0, max: 0.0\n",
      "Frame 37: mask min: 0.0, max: 0.0\n",
      "Frame 38: mask min: 0.0, max: 0.0\n",
      "Frame 39: mask min: 0.0, max: 0.0\n",
      "Frame 40: mask min: 0.0, max: 0.0\n",
      "Frame 41: mask min: 0.0, max: 0.0\n",
      "Frame 42: mask min: 0.0, max: 0.0\n",
      "Frame 43: mask min: 0.0, max: 0.0\n",
      "Frame 44: mask min: 0.0, max: 0.0\n",
      "Frame 45: mask min: 0.0, max: 0.0\n",
      "Frame 46: mask min: 0.0, max: 0.0\n",
      "Frame 47: mask min: 0.0, max: 0.0\n",
      "Frame 48: mask min: 0.0, max: 0.0\n",
      "Frame 49: mask min: 0.0, max: 0.0\n",
      "Frame 50: mask min: 0.0, max: 0.0\n",
      "Frame 51: mask min: 0.0, max: 0.0\n",
      "Frame 52: mask min: 0.0, max: 0.0\n",
      "Frame 53: mask min: 0.0, max: 0.0\n",
      "Frame 54: mask min: 0.0, max: 0.0\n",
      "Frame 55: mask min: 0.0, max: 0.0\n",
      "Frame 56: mask min: 0.0, max: 0.0\n",
      "Frame 57: mask min: 0.0, max: 0.0\n",
      "Frame 58: mask min: 0.0, max: 0.0\n",
      "Frame 59: mask min: 0.0, max: 0.0\n",
      "Frame 60: mask min: 0.0, max: 0.0\n",
      "Frame 61: mask min: 0.0, max: 0.0\n",
      "Frame 62: mask min: 0.0, max: 0.0\n",
      "Frame 63: mask min: 0.0, max: 0.0\n",
      "Frame 64: mask min: 0.0, max: 0.0\n",
      "Frame 65: mask min: 0.0, max: 0.0\n",
      "Frame 66: mask min: 0.0, max: 0.0\n",
      "Frame 67: mask min: 0.0, max: 0.0\n",
      "Frame 68: mask min: 0.0, max: 0.0\n",
      "Frame 69: mask min: 0.0, max: 0.0\n",
      "Frame 70: mask min: 0.0, max: 0.0\n",
      "Frame 71: mask min: 0.0, max: 0.0\n",
      "Frame 72: mask min: 0.0, max: 0.0\n",
      "Frame 73: mask min: 0.0, max: 0.0\n",
      "Frame 74: mask min: 0.0, max: 0.0\n",
      "Frame 75: mask min: 0.0, max: 0.0\n",
      "Frame 76: mask min: 0.0, max: 0.0\n",
      "Frame 77: mask min: 0.0, max: 0.0\n",
      "Frame 78: mask min: 0.0, max: 0.0\n",
      "Frame 79: mask min: 0.0, max: 0.0\n",
      "Frame 80: mask min: 0.0, max: 0.0\n",
      "Frame 81: mask min: 0.0, max: 0.0\n",
      "Frame 82: mask min: 0.0, max: 0.0\n",
      "Frame 83: mask min: 0.0, max: 0.0\n",
      "Frame 84: mask min: 0.0, max: 0.0\n",
      "Frame 85: mask min: 0.0, max: 0.0\n",
      "Frame 86: mask min: 0.0, max: 0.0\n",
      "Frame 87: mask min: 0.0, max: 0.0\n",
      "Frame 88: mask min: 0.0, max: 0.0\n",
      "Frame 89: mask min: 0.0, max: 0.0\n",
      "Frame 90: mask min: 0.0, max: 0.0\n",
      "Frame 91: mask min: 0.0, max: 0.0\n",
      "Frame 92: mask min: 0.0, max: 0.0\n",
      "Frame 93: mask min: 0.0, max: 0.0\n",
      "Frame 94: mask min: 0.0, max: 0.0\n",
      "Frame 95: mask min: 0.0, max: 0.0\n",
      "Frame 96: mask min: 0.0, max: 0.0\n",
      "Frame 97: mask min: 0.0, max: 0.0\n",
      "Frame 98: mask min: 0.0, max: 0.0\n",
      "Frame 99: mask min: 0.0, max: 0.0\n",
      "Frame 100: mask min: 0.0, max: 0.0\n",
      "Frame 101: mask min: 0.0, max: 0.0\n",
      "Frame 102: mask min: 0.0, max: 0.0\n",
      "Frame 103: mask min: 0.0, max: 0.0\n",
      "Frame 104: mask min: 0.0, max: 0.0\n",
      "Frame 105: mask min: 0.0, max: 0.0\n",
      "Frame 106: mask min: 0.0, max: 0.0\n",
      "Frame 107: mask min: 0.0, max: 0.0\n",
      "Frame 108: mask min: 0.0, max: 0.0\n",
      "Frame 109: mask min: 0.0, max: 0.0\n",
      "Frame 110: mask min: 0.0, max: 0.0\n",
      "Frame 111: mask min: 0.0, max: 0.0\n",
      "Frame 112: mask min: 0.0, max: 0.0\n",
      "Frame 113: mask min: 0.0, max: 0.0\n",
      "Frame 114: mask min: 0.0, max: 0.0\n",
      "Frame 115: mask min: 0.0, max: 0.0\n",
      "Frame 116: mask min: 0.0, max: 0.0\n",
      "Frame 117: mask min: 0.0, max: 0.0\n",
      "Frame 118: mask min: 0.0, max: 0.0\n",
      "Frame 119: mask min: 0.0, max: 0.0\n",
      "Frame 120: mask min: 0.0, max: 0.0\n",
      "Frame 121: mask min: 0.0, max: 0.0\n",
      "Frame 122: mask min: 0.0, max: 0.0\n",
      "Frame 123: mask min: 0.0, max: 0.0\n",
      "Frame 124: mask min: 0.0, max: 0.0\n",
      "Frame 125: mask min: 0.0, max: 0.0\n",
      "Frame 126: mask min: 0.0, max: 0.0\n",
      "Frame 127: mask min: 0.0, max: 0.0\n",
      "Frame 128: mask min: 0.0, max: 0.0\n",
      "Frame 129: mask min: 0.0, max: 0.0\n",
      "Frame 130: mask min: 0.0, max: 0.0\n",
      "Frame 131: mask min: 0.0, max: 0.0\n",
      "Frame 132: mask min: 0.0, max: 0.0\n",
      "Frame 133: mask min: 0.0, max: 0.0\n",
      "Frame 134: mask min: 0.0, max: 0.0\n",
      "Frame 135: mask min: 0.0, max: 0.0\n",
      "Frame 136: mask min: 0.0, max: 0.0\n",
      "Frame 137: mask min: 0.0, max: 0.0\n",
      "Frame 138: mask min: 0.0, max: 0.0\n",
      "Frame 139: mask min: 0.0, max: 0.0\n",
      "Frame 140: mask min: 0.0, max: 0.0\n",
      "Frame 141: mask min: 0.0, max: 0.0\n",
      "Frame 142: mask min: 0.0, max: 0.0\n",
      "Frame 143: mask min: 0.0, max: 0.0\n",
      "Frame 144: mask min: 0.0, max: 0.0\n",
      "Video saved at optical_flow/annotation_824/masked_1.2.826.0.1.3680043.8.498.19685680876768794543773205326660692566.mp4\n",
      "Tracking and saving videos completed.\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Minimum number of good points to continue tracking\n",
    "MIN_GOOD_POINTS = 8\n",
    "MIN_CONFIDENCE = 0.9\n",
    "MIN_MASK_AREA = 100  # Minimum area of the mask to consider it valid\n",
    "MASK_INTENSITY_THRESHOLD = 0.2  # Minimum average intensity of the mask\n",
    "\n",
    "def apply_optical_flow(prev_frame, curr_frame, prev_mask):\n",
    "    # Calculate dense optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_frame, curr_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Create a meshgrid of coordinates\n",
    "    h, w = prev_frame.shape[:2]\n",
    "    y, x = np.mgrid[0:h, 0:w].reshape(2, -1).astype(int)\n",
    "    \n",
    "    # Apply the flow to the coordinates\n",
    "    fx, fy = flow[y, x].T\n",
    "    coords = np.vstack([x + fx, y + fy]).round().astype(int)\n",
    "    \n",
    "    # Clip the coordinates to stay within the image\n",
    "    coords[0] = np.clip(coords[0], 0, w - 1)\n",
    "    coords[1] = np.clip(coords[1], 0, h - 1)\n",
    "    \n",
    "    # Create the new mask\n",
    "    new_mask = np.zeros_like(prev_mask, dtype=float)\n",
    "    new_mask[coords[1], coords[0]] = prev_mask[y, x]\n",
    "    \n",
    "    # Apply some morphological operations to clean up the mask\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    new_mask = cv2.morphologyEx(new_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    new_mask = cv2.morphologyEx(new_mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return new_mask\n",
    "\n",
    "def track_frames(cap, start_frame, end_frame, initial_mask, forward=True):\n",
    "    frames = []\n",
    "    step = 1 if forward else -1\n",
    "    frame_idx = start_frame\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        return frames\n",
    "    \n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    mask = initial_mask.astype(float)\n",
    "    \n",
    "    while (forward and frame_idx <= end_frame) or (not forward and frame_idx >= 0):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # For the initial frame, use the initial mask\n",
    "        if frame_idx == start_frame:\n",
    "            new_mask = mask\n",
    "        else:\n",
    "            # Apply optical flow\n",
    "            new_mask = apply_optical_flow(prev_gray, frame_gray, mask)\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(f\"Frame {frame_idx}: mask min: {new_mask.min()}, max: {new_mask.max()}\")\n",
    "        \n",
    "        # Apply thresholding\n",
    "        mask_area = np.sum(new_mask > MASK_INTENSITY_THRESHOLD)\n",
    "        mask_avg_intensity = np.mean(new_mask)\n",
    "        \n",
    "        if mask_area < MIN_MASK_AREA or mask_avg_intensity < MASK_INTENSITY_THRESHOLD:\n",
    "            new_mask = np.zeros_like(new_mask)\n",
    "        else:\n",
    "            # Normalize the mask to [0, 1] range\n",
    "            new_mask = (new_mask > MASK_INTENSITY_THRESHOLD).astype(float)\n",
    "        \n",
    "        # Create annotated frame\n",
    "        mask_overlay = np.zeros_like(frame, dtype=float)\n",
    "        mask_overlay[:,:,2] = new_mask  # Red channel\n",
    "        annotated_frame = cv2.addWeighted(frame, 1, (mask_overlay * 255).astype(np.uint8), 0.5, 0)\n",
    "        \n",
    "        frames.append((frame_idx, annotated_frame, new_mask))\n",
    "        \n",
    "        prev_gray = frame_gray.copy()\n",
    "        mask = new_mask\n",
    "        frame_idx += step\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def save_combined_video(video_path, output_video_path, initial_mask, frame_number):\n",
    "    save_dir = os.path.dirname(output_video_path)\n",
    "    mask_dir = os.path.join(save_dir, \"masks\")\n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Track backward from the annotated frame to the start\n",
    "    backward_frames = track_frames(cap, frame_number, 0, initial_mask, forward=False)\n",
    "    # Track forward from the annotated frame to the end\n",
    "    forward_frames = track_frames(cap, frame_number, total_frames - 1, initial_mask, forward=True)\n",
    "    \n",
    "    # Combine backward and forward frames\n",
    "    combined_frames = backward_frames[::-1] + forward_frames[1:]\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Write combined frames to the video\n",
    "    for frame_idx, frame, mask in combined_frames:\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Save mask as separate image\n",
    "        mask_filename = os.path.join(mask_dir, f\"mask_{frame_idx:04d}.png\")\n",
    "        cv2.imwrite(mask_filename, (mask * 255).astype(np.uint8))\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "\n",
    "def track_and_save_masks_as_video(annotation, output_dir):\n",
    "    video_id = annotation['SeriesInstanceUID']\n",
    "    video_path = annotation['video_path']\n",
    "    frame_number = int(annotation['frameNumber'])\n",
    "    foreground = annotation['data']['foreground']\n",
    "\n",
    "    print(f\"Processing Video: {video_id}; Frame: {frame_number}...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "    cap.release()\n",
    "    \n",
    "    initial_mask = polygons_to_mask(foreground, frame.shape[0], frame.shape[1])\n",
    "    print(f\"Initial mask shape: {initial_mask.shape}\")\n",
    "    print(f\"Initial mask min: {initial_mask.min()}, max: {initial_mask.max()}\")\n",
    "    cv2.imwrite(os.path.join(output_dir, 'initial_mask.png'), initial_mask * 255)\n",
    "    \n",
    "    output_video_path = os.path.join(output_dir, f'masked_{video_id}.mp4')\n",
    "    save_combined_video(video_path, output_video_path, initial_mask, frame_number)\n",
    "\n",
    "\n",
    "# Apply the tracking function to each annotation in random_annotations\n",
    "output_base_dir = 'optical_flow'  # Base directory to save the tracked videos\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for index, annotation in random_annotations.iterrows():\n",
    "    output_dir = os.path.join(output_base_dir, f'annotation_{index}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    track_and_save_masks_as_video(annotation, output_dir)\n",
    "\n",
    "print(\"Tracking and saving videos completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
