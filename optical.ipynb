{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set constants\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mdai\n",
    "from mdai.visualize import load_mask, display_images\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "load_dotenv('dot.env')\n",
    "\n",
    "ACCESS_TOKEN = os.getenv('MDAI_TOKEN')\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "DOMAIN = os.getenv('DOMAIN')\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "DATASET_ID = os.getenv('DATASET_ID')\n",
    "ANNOTATIONS = os.path.join(DATA_DIR, os.getenv('ANNOTATIONS'))\n",
    "LABEL_ID = os.getenv('LABEL_ID')\n",
    "\n",
    "# Define an error threshold to filter out low-confidence points\n",
    "ERROR_THRESHOLD = 1.0\n",
    "\n",
    "print(f\"ACCESS_TOKEN={ACCESS_TOKEN}\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"DOMAIN={DOMAIN}\")\n",
    "print(f\"PROJECT_ID={PROJECT_ID}\")\n",
    "print(f\"DATASET_ID={DATASET_ID}\")\n",
    "print(f\"ANNOTATIONS={ANNOTATIONS}\")\n",
    "print(f\"LABEL_ID={LABEL_ID}\")\n",
    "\n",
    "# Start MD.ai client\n",
    "mdai_client = mdai.Client(domain=DOMAIN, access_token=ACCESS_TOKEN)\n",
    "\n",
    "# Download the dataset from MD.ai (or use cached version)\n",
    "project = mdai_client.project(project_id=PROJECT_ID, path=DATA_DIR)\n",
    "\n",
    "# Load the annotations\n",
    "annotations_data = mdai.common_utils.json_to_dataframe(ANNOTATIONS)\n",
    "annotations_df = pd.DataFrame(annotations_data['annotations'])\n",
    "labels = annotations_df['labelId'].unique()\n",
    "\n",
    "# Create the label map, LABEL_ID => 1, others in labels => 0\n",
    "labels_dict = {LABEL_ID: 1}\n",
    "project.set_labels_dict(labels_dict)\n",
    "\n",
    "# Get the dataset\n",
    "dataset = project.get_dataset_by_id(DATASET_ID)\n",
    "dataset.classes_dict = project.classes_dict \n",
    "\n",
    "# Ensure BASE is set after preparing the dataset\n",
    "BASE = dataset.images_dir\n",
    "\n",
    "# Filter annotations for the free fluid label\n",
    "free_fluid_annotations = annotations_df[annotations_df['labelId'] == LABEL_ID].copy()\n",
    "\n",
    "# Function to construct the video path\n",
    "def construct_video_path(base_dir, study_uid, series_uid):\n",
    "    return os.path.join(base_dir, study_uid, f\"{series_uid}.mp4\")\n",
    "\n",
    "# Add video paths to the dataframe using .loc to avoid the SettingWithCopyWarning\n",
    "free_fluid_annotations['video_path'] = free_fluid_annotations.apply(\n",
    "    lambda row: construct_video_path(BASE, row['StudyInstanceUID'], row['SeriesInstanceUID']), axis=1)\n",
    "\n",
    "# Check if video files exist and add the result to the dataframe using .loc\n",
    "free_fluid_annotations['file_exists'] = free_fluid_annotations['video_path'].apply(os.path.exists)\n",
    "\n",
    "# Count the number of annotations with and without corresponding video files\n",
    "num_with_files = free_fluid_annotations['file_exists'].sum()\n",
    "num_without_files = len(free_fluid_annotations) - num_with_files\n",
    "\n",
    "print(f\"Annotations with corresponding video files: {num_with_files}\")\n",
    "print(f\"Annotations without corresponding video files: {num_without_files}\")\n",
    "\n",
    "# Select five random annotations with corresponding video files\n",
    "if DEBUG:\n",
    "    random_annotations = free_fluid_annotations[free_fluid_annotations['file_exists']].sample(n=1, random_state=42)\n",
    "else:\n",
    "    random_annotations = free_fluid_annotations[free_fluid_annotations['file_exists']].sample(n=5, random_state=42)\n",
    "\n",
    "# Display function\n",
    "def polygons_to_mask(polygons, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    for polygon in polygons:\n",
    "        points = np.array(polygon, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "def display_annotation(row):\n",
    "    video_path = row['video_path']\n",
    "    frame_number = int(row['frameNumber'])\n",
    "    foreground = row['data']['foreground']\n",
    "    video_id = row['SeriesInstanceUID']\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "\n",
    "    mask = polygons_to_mask(foreground, frame.shape[0], frame.shape[1])\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame[mask == 1] = (0, 0, 255)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    ax[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    ax[0].set_title(f'Video ID: {video_id}')\n",
    "    ax[1].imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
    "    # ax[1].set_title(f'Annotated Frame (Video ID: {video_id}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Mask size and intensity thresholds\n",
    "MASK_MIN_SIZE = 100\n",
    "INTENSITY_THRESHOLD = 30\n",
    "\n",
    "def apply_optical_flow(prev_frame, curr_frame, prev_mask):\n",
    "    # Calculate dense optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_frame, curr_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Create a meshgrid of coordinates\n",
    "    h, w = prev_frame.shape[:2]\n",
    "    y, x = np.mgrid[0:h, 0:w].reshape(2, -1).astype(int)\n",
    "    \n",
    "    # Apply the flow to the coordinates\n",
    "    fx, fy = flow[y, x].T\n",
    "    coords = np.vstack([x + fx, y + fy]).round().astype(int)\n",
    "    \n",
    "    # Clip the coordinates to stay within the image\n",
    "    coords[0] = np.clip(coords[0], 0, w - 1)\n",
    "    coords[1] = np.clip(coords[1], 0, h - 1)\n",
    "    \n",
    "    # Create the new mask\n",
    "    new_mask = np.zeros_like(prev_mask, dtype=float)\n",
    "    new_mask[coords[1], coords[0]] = prev_mask[y, x]\n",
    "    \n",
    "    # Apply some morphological operations to clean up the mask\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    new_mask = cv2.morphologyEx(new_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    new_mask = cv2.morphologyEx(new_mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return new_mask\n",
    "\n",
    "def trim_and_threshold_mask(frame, mask):\n",
    "    # Invert the frame so that dark areas (potential fluid) have high values\n",
    "    inverted_frame = 255 - frame\n",
    "    \n",
    "    # Create a binary mask based on the intensity threshold\n",
    "    intensity_mask = inverted_frame > INTENSITY_THRESHOLD\n",
    "    \n",
    "    # Combine the optical flow mask with the intensity mask\n",
    "    trimmed_mask = (mask > 0) & intensity_mask\n",
    "    \n",
    "    # Check if the mask is too small\n",
    "    if np.sum(trimmed_mask) < MASK_MIN_SIZE:\n",
    "        return np.zeros_like(mask)\n",
    "    \n",
    "    return trimmed_mask.astype(float)\n",
    "\n",
    "def track_frames(cap, start_frame, end_frame, initial_mask, forward=True):\n",
    "    frames = []\n",
    "    step = 1 if forward else -1\n",
    "    frame_idx = start_frame\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        return frames\n",
    "    \n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    mask = initial_mask.astype(float)\n",
    "    \n",
    "    while (forward and frame_idx <= end_frame) or (not forward and frame_idx >= 0):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # For the initial frame, use the initial mask\n",
    "        if frame_idx == start_frame:\n",
    "            new_mask = mask\n",
    "        else:\n",
    "            # Apply optical flow\n",
    "            new_mask = apply_optical_flow(prev_gray, frame_gray, mask)\n",
    "        \n",
    "        # Trim low-intensity pixels and apply size threshold\n",
    "        new_mask = trim_and_threshold_mask(frame_gray, new_mask)\n",
    "        \n",
    "        # Create annotated frame\n",
    "        mask_overlay = np.zeros_like(frame, dtype=float)\n",
    "        mask_overlay[:,:,1] = new_mask\n",
    "        annotated_frame = cv2.addWeighted(frame, 1, (mask_overlay * 255).astype(np.uint8), 0.5, 0)\n",
    "        \n",
    "        # Add mask area text to the frame\n",
    "        mask_area = np.sum(new_mask > 0)\n",
    "        if DEBUG:\n",
    "            cv2.putText(annotated_frame, f\"Mask area: {mask_area}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        frames.append((frame_idx, annotated_frame, new_mask))\n",
    "        \n",
    "        prev_gray = frame_gray.copy()\n",
    "        mask = new_mask\n",
    "        frame_idx += step\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def save_combined_video(video_path, output_video_path, initial_mask, frame_number):\n",
    "    save_dir = os.path.dirname(output_video_path)\n",
    "    mask_dir = os.path.join(save_dir, \"masks\")\n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Track backward from the annotated frame to the start\n",
    "    backward_frames = track_frames(cap, frame_number, 0, initial_mask, forward=False)\n",
    "    # Track forward from the annotated frame to the end\n",
    "    forward_frames = track_frames(cap, frame_number, total_frames - 1, initial_mask, forward=True)\n",
    "    \n",
    "    # Combine backward and forward frames\n",
    "    combined_frames = backward_frames[::-1] + forward_frames[1:]\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Write combined frames to the video\n",
    "    for frame_idx, frame, mask in combined_frames:\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Save mask as separate image\n",
    "        mask_filename = os.path.join(mask_dir, f\"mask_{frame_idx:04d}.png\")\n",
    "        cv2.imwrite(mask_filename, (mask * 255).astype(np.uint8))\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "\n",
    "def track_and_save_masks_as_video(annotation, output_dir):\n",
    "    video_id = annotation['SeriesInstanceUID']\n",
    "    video_path = annotation['video_path']\n",
    "    frame_number = int(annotation['frameNumber'])\n",
    "    foreground = annotation['data']['foreground']\n",
    "\n",
    "    print(f\"Processing Video: {video_id}; Frame: {frame_number}...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "    cap.release()\n",
    "    \n",
    "    initial_mask = polygons_to_mask(foreground, frame.shape[0], frame.shape[1])\n",
    "    print(f\"Initial mask shape: {initial_mask.shape}\")\n",
    "    print(f\"Initial mask min: {initial_mask.min()}, max: {initial_mask.max()}\")\n",
    "    cv2.imwrite(os.path.join(output_dir, f'initial_mask_frame_{frame_number}.png'), initial_mask * 255)\n",
    "    \n",
    "    output_video_path = os.path.join(output_dir, f'masked_{video_id}.mp4')\n",
    "    save_combined_video(video_path, output_video_path, initial_mask, frame_number)\n",
    "\n",
    "\n",
    "# Apply the tracking function to each annotation in random_annotations\n",
    "output_base_dir = 'optical_flow'  # Base directory to save the tracked videos\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for index, annotation in random_annotations.iterrows():\n",
    "    output_dir = os.path.join(output_base_dir, f'annotation_{index}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    track_and_save_masks_as_video(annotation, output_dir)\n",
    "\n",
    "print(\"Tracking and saving videos completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
