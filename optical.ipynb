{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set constants\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mdai\n",
    "from mdai.visualize import load_mask, display_images\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "load_dotenv('dot.env')\n",
    "\n",
    "ACCESS_TOKEN = os.getenv('MDAI_TOKEN')\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "DOMAIN = os.getenv('DOMAIN')\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "DATASET_ID = os.getenv('DATASET_ID')\n",
    "ANNOTATIONS = os.path.join(DATA_DIR, os.getenv('ANNOTATIONS'))\n",
    "LABEL_ID = os.getenv('LABEL_ID')\n",
    "\n",
    "# Define an error threshold to filter out low-confidence points\n",
    "ERROR_THRESHOLD = 1.0\n",
    "\n",
    "print(f\"ACCESS_TOKEN={ACCESS_TOKEN}\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"DOMAIN={DOMAIN}\")\n",
    "print(f\"PROJECT_ID={PROJECT_ID}\")\n",
    "print(f\"DATASET_ID={DATASET_ID}\")\n",
    "print(f\"ANNOTATIONS={ANNOTATIONS}\")\n",
    "print(f\"LABEL_ID={LABEL_ID}\")\n",
    "\n",
    "# Start MD.ai client\n",
    "mdai_client = mdai.Client(domain=DOMAIN, access_token=ACCESS_TOKEN)\n",
    "\n",
    "# Download the dataset from MD.ai (or use cached version)\n",
    "project = mdai_client.project(project_id=PROJECT_ID, path=DATA_DIR)\n",
    "\n",
    "# Load the annotations\n",
    "annotations_data = mdai.common_utils.json_to_dataframe(ANNOTATIONS)\n",
    "annotations_df = pd.DataFrame(annotations_data['annotations'])\n",
    "labels = annotations_df['labelId'].unique()\n",
    "\n",
    "# Create the label map, LABEL_ID => 1, others in labels => 0\n",
    "labels_dict = {LABEL_ID: 1}\n",
    "project.set_labels_dict(labels_dict)\n",
    "\n",
    "# Get the dataset\n",
    "dataset = project.get_dataset_by_id(DATASET_ID)\n",
    "dataset.classes_dict = project.classes_dict \n",
    "\n",
    "# Ensure BASE is set after preparing the dataset\n",
    "BASE = dataset.images_dir\n",
    "\n",
    "# Filter annotations for the free fluid label\n",
    "free_fluid_annotations = annotations_df[annotations_df['labelId'] == LABEL_ID].copy()\n",
    "\n",
    "# Function to construct the video path\n",
    "def construct_video_path(base_dir, study_uid, series_uid):\n",
    "    return os.path.join(base_dir, study_uid, f\"{series_uid}.mp4\")\n",
    "\n",
    "# Add video paths to the dataframe using .loc to avoid the SettingWithCopyWarning\n",
    "free_fluid_annotations['video_path'] = free_fluid_annotations.apply(\n",
    "    lambda row: construct_video_path(BASE, row['StudyInstanceUID'], row['SeriesInstanceUID']), axis=1)\n",
    "\n",
    "# Check if video files exist and add the result to the dataframe using .loc\n",
    "free_fluid_annotations['file_exists'] = free_fluid_annotations['video_path'].apply(os.path.exists)\n",
    "\n",
    "# Count the number of annotations with and without corresponding video files\n",
    "num_with_files = free_fluid_annotations['file_exists'].sum()\n",
    "num_without_files = len(free_fluid_annotations) - num_with_files\n",
    "\n",
    "print(f\"Annotations with corresponding video files: {num_with_files}\")\n",
    "print(f\"Annotations without corresponding video files: {num_without_files}\")\n",
    "\n",
    "# Select five random annotations with corresponding video files\n",
    "random_annotations = free_fluid_annotations[free_fluid_annotations['file_exists']].sample(n=5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Display function\n",
    "def polygons_to_mask(polygons, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    for polygon in polygons:\n",
    "        points = np.array(polygon, dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "def display_annotation(row):\n",
    "    video_path = row['video_path']\n",
    "    frame_number = int(row['frameNumber'])\n",
    "    foreground = row['data']['foreground']\n",
    "    video_id = row['SeriesInstanceUID']\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "\n",
    "    mask = polygons_to_mask(foreground, frame.shape[0], frame.shape[1])\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    annotated_frame[mask == 1] = (0, 0, 255)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    ax[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    ax[0].set_title(f'Video ID: {video_id}')\n",
    "    ax[1].imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
    "    # ax[1].set_title(f'Annotated Frame (Video ID: {video_id}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display selected annotations\n",
    "for _, row in random_annotations.iterrows():\n",
    "    display_annotation(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "def sort_points(points):\n",
    "    # Calculate the centroid of the points\n",
    "    center = np.mean(points, axis=0)\n",
    "    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n",
    "    return points[np.argsort(angles)]\n",
    "\n",
    "def track_frames(cap, start_frame, end_frame, p0, initial_mask, forward=True):\n",
    "    frames = []\n",
    "\n",
    "    step = 1 if forward else -1\n",
    "    frame_idx = start_frame\n",
    "\n",
    "    old_frame = None\n",
    "    old_gray = None\n",
    "\n",
    "    while (forward and frame_idx <= end_frame) or (not forward and frame_idx >= 1):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if old_frame is None:\n",
    "            old_frame = frame\n",
    "            old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "            if initial_mask.shape != old_gray.shape:\n",
    "                initial_mask = cv2.resize(initial_mask, (old_gray.shape[1], old_gray.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "            p0 = cv2.goodFeaturesToTrack(old_gray, mask=initial_mask, **feature_params)\n",
    "            frame_idx += step\n",
    "            continue\n",
    "\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        if p1 is None or st is None:\n",
    "            print(f\"Failed to compute optical flow at frame {frame_idx}.\")\n",
    "            break\n",
    "\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "\n",
    "        mask = np.zeros_like(frame_gray)\n",
    "        if len(good_new) > 0:\n",
    "            points = good_new.reshape(-1, 2).astype(np.int32)\n",
    "            sorted_points = sort_points(points)\n",
    "            cv2.fillPoly(mask, [sorted_points], 255)\n",
    "\n",
    "        mask_3ch = cv2.merge([mask, np.zeros_like(mask), np.zeros_like(mask)])  # Blue mask\n",
    "        annotated_frame = cv2.addWeighted(frame, 1, mask_3ch, 0.5, 0)\n",
    "        frames.append(annotated_frame)\n",
    "\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "        frame_idx += step\n",
    "\n",
    "    return frames\n",
    "\n",
    "def save_combined_video(video_path, output_video_path, initial_mask, frame_number):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Track backward from the annotated frame to the start\n",
    "    backward_frames = track_frames(cap, frame_number, 1, None, initial_mask, forward=False)\n",
    "    # Track forward from the annotated frame to the end\n",
    "    forward_frames = track_frames(cap, frame_number, total_frames - 1, None, initial_mask, forward=True)\n",
    "    \n",
    "    # Combine backward and forward frames, ensuring they are in the correct order\n",
    "    combined_frames = backward_frames[::-1] + forward_frames[1:]  # Reverse backward frames and skip the duplicated annotated frame\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    for frame in combined_frames:\n",
    "        out.write(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "\n",
    "def track_and_save_masks_as_video(annotation, output_dir):\n",
    "    video_id = annotation['SeriesInstanceUID']\n",
    "    video_path = annotation['video_path']\n",
    "    frame_number = int(annotation['frameNumber'])\n",
    "    foreground = annotation['data']['foreground']\n",
    "    \n",
    "    # Read the first frame to get the size\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "    cap.release()\n",
    "    \n",
    "    # Initialize mask from polygons with the correct size\n",
    "    initial_mask = polygons_to_mask(foreground, frame.shape[0], frame.shape[1])\n",
    "    \n",
    "    # Perform tracking and save masked frames as masked_{video_id}.mp4\n",
    "    output_video_path = os.path.join(output_dir, f'masked_{video_id}.mp4')\n",
    "    save_combined_video(video_path, output_video_path, initial_mask, frame_number)\n",
    "\n",
    "# Apply the tracking function to each annotation in random_annotations\n",
    "output_base_dir = 'tracked_videos'  # Base directory to save the tracked videos\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for index, annotation in random_annotations.iterrows():\n",
    "    output_dir = os.path.join(output_base_dir, f'annotation_{index}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    track_and_save_masks_as_video(annotation, output_dir)\n",
    "\n",
    "print(\"Tracking and saving videos completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Minimum number of good points to continue tracking\n",
    "MIN_GOOD_POINTS = 8\n",
    "MIN_CONFIDENCE = 0.9\n",
    "MIN_MASK_AREA = 500  # Define your minimum mask area\n",
    "\n",
    "def sort_points(points):\n",
    "    center = np.mean(points, axis=0)\n",
    "    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n",
    "    return points[np.argsort(angles)]\n",
    "\n",
    "def track_frames(cap, start_frame, end_frame, p0, initial_mask, forward=True, save_dir=None):\n",
    "    frames = []\n",
    "    step = 1 if forward else -1\n",
    "    dir = \"forward\" if forward else \"backward\"\n",
    "    frame_idx = start_frame\n",
    "    old_frame = None\n",
    "    old_gray = None\n",
    "\n",
    "    while (forward and frame_idx <= end_frame) or (not forward and frame_idx >= 1):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if old_frame is None:\n",
    "            old_frame = frame\n",
    "            old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "            if initial_mask.shape != old_gray.shape:\n",
    "                initial_mask = cv2.resize(initial_mask, (old_gray.shape[1], old_gray.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "            p0 = cv2.goodFeaturesToTrack(old_gray, mask=initial_mask, **feature_params)\n",
    "            frame_idx += step\n",
    "            continue\n",
    "\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        if p1 is None or st is None:\n",
    "            print(f\"Stopping {dir} tracking at frame {frame_idx} due to optical flow calculation failure.\")\n",
    "            break\n",
    "\n",
    "        high_confidence_idx = np.where(st >= MIN_CONFIDENCE)[0]\n",
    "        good_new = p1[high_confidence_idx]\n",
    "        good_old = p0[high_confidence_idx]\n",
    "\n",
    "        if len(good_new) < MIN_GOOD_POINTS:\n",
    "            print(f\"Stopping {dir} tracking at frame {frame_idx} due to insufficient good points.\")\n",
    "            break\n",
    "\n",
    "        mask = np.zeros_like(frame_gray)\n",
    "        if len(good_new) > 0:\n",
    "            points = good_new.reshape(-1, 2).astype(np.int32)\n",
    "            sorted_points = sort_points(points)\n",
    "            cv2.fillPoly(mask, [sorted_points], 255)\n",
    "\n",
    "            if cv2.countNonZero(mask) < MIN_MASK_AREA:\n",
    "                print(f\"Stopping {dir} tracking at frame {frame_idx} due to mask area below threshold.\")\n",
    "                break\n",
    "\n",
    "        if save_dir:\n",
    "            mask_filename = os.path.join(save_dir, f\"mask_{frame_idx:04d}.png\")\n",
    "            cv2.imwrite(mask_filename, mask)\n",
    "        \n",
    "        mask_3ch = cv2.merge([mask, np.zeros_like(mask), np.zeros_like(mask)])  # Blue mask\n",
    "        annotated_frame = cv2.addWeighted(frame, 1, mask_3ch, 0.5, 0)\n",
    "        frames.append((frame_idx, annotated_frame))\n",
    "\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "        frame_idx += step\n",
    "\n",
    "    return frames\n",
    "\n",
    "def save_combined_video(video_path, output_video_path, initial_mask, frame_number, save_dir=None):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(save_dir)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Track backward from the annotated frame to the start\n",
    "    backward_frames = track_frames(cap, frame_number, 1, None, initial_mask, forward=False, save_dir=save_dir)\n",
    "    # Track forward from the annotated frame to the end\n",
    "    forward_frames = track_frames(cap, frame_number, total_frames - 1, None, initial_mask, forward=True, save_dir=save_dir)\n",
    "    \n",
    "    # Combine backward and forward frames, ensuring they are in the correct order\n",
    "    combined_frames = backward_frames[::-1] + forward_frames[1:]  # Reverse backward frames and skip the duplicated annotated frame\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Write combined frames to the video\n",
    "    if combined_frames:\n",
    "        for frame_idx, frame in combined_frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        # Include unmasked frames after early stopping\n",
    "        if len(combined_frames) < total_frames:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, combined_frames[-1][0] + 1)\n",
    "            for _ in range(len(combined_frames), total_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                out.write(frame)\n",
    "    else:\n",
    "        # If combined_frames is empty, write the original frames\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        for _ in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "\n",
    "def track_and_save_masks_as_video(annotation, output_dir):\n",
    "    video_id = annotation['SeriesInstanceUID']\n",
    "    video_path = annotation['video_path']\n",
    "    frame_number = int(annotation['frameNumber'])\n",
    "    foreground = annotation['data']['foreground']\n",
    "\n",
    "    # Announce the start of tracking\n",
    "    print(f\"Video: {video_id}; Frame: {frame_number}...\")\n",
    "    \n",
    "    # Read the first frame to get the size\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "    cap.release()\n",
    "    \n",
    "    # Initialize mask from polygons with the correct size\n",
    "    initial_mask = polygons_to_mask(foreground, frame.shape[0], frame.shape[1])\n",
    "    \n",
    "    # Perform tracking and save masked frames as masked_{video_id}.mp4\n",
    "    output_video_path = os.path.join(output_dir, f'masked_{video_id}.mp4')\n",
    "    save_dir = os.path.join(output_dir, 'masks')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_combined_video(video_path, output_video_path, initial_mask, frame_number, save_dir=save_dir)\n",
    "\n",
    "# Apply the tracking function to each annotation in random_annotations\n",
    "output_base_dir = 'tracked_videos'  # Base directory to save the tracked videos\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for index, annotation in random_annotations.iterrows():\n",
    "    output_dir = os.path.join(output_base_dir, f'annotation_{index}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    track_and_save_masks_as_video(annotation, output_dir)\n",
    "\n",
    "print(\"Tracking and saving videos completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature parameters for goodFeaturesToTrack\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Define parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "def track_free_fluid(annotation):\n",
    "    video_path = annotation['video_path']\n",
    "    frame_number = int(annotation['frameNumber'])\n",
    "    foreground = annotation['data']['foreground']\n",
    "\n",
    "    print(f\"Processing video: {video_path}, frame: {frame_number}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Move to the annotated frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, old_frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Combine all shapes into a single list of points\n",
    "    all_points = [point for shape in foreground for point in shape]\n",
    "    p0 = np.array(all_points, np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "    # print(f\"Initial points for tracking (p0): {p0}\")\n",
    "\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"No more frames to read or end of video reached.\")\n",
    "            break\n",
    "\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate optical flow\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        if p1 is None or st is None:\n",
    "            print(\"Optical flow failed or no features found.\")\n",
    "            break\n",
    "\n",
    "        # Select good points\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "\n",
    "        print(f\"New good points: {good_new}\")\n",
    "        print(f\"Old good points: {good_old}\")\n",
    "\n",
    "        if len(good_new) == 0 or len(good_old) == 0:\n",
    "            print(\"No good points found.\")\n",
    "            break\n",
    "\n",
    "        # Draw the tracks\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            print(f\"Drawing line: start=({a}, {b}), end=({c}, {d})\")\n",
    "            a, b, c, d = int(a), int(b), int(c), int(d)\n",
    "            frame = cv2.line(frame, (a, b), (c, d), (0, 255, 0), 2)\n",
    "            frame = cv2.circle(frame, (a, b), 5, (0, 255, 0), -1)\n",
    "\n",
    "        img = frame\n",
    "\n",
    "        cv2.imshow('frame', img)\n",
    "        if cv2.waitKey(30) & 0xFF == 27:\n",
    "            print(\"Stopped by user.\")\n",
    "            break\n",
    "\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "for _, annotation in random_annotations.iterrows():\n",
    "    display_annotation(annotation)\n",
    "    track_free_fluid(annotation)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define feature parameters for goodFeaturesToTrack\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Define parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Confidence threshold for dropping points with high error\n",
    "confidence_threshold = 1.0\n",
    "\n",
    "def track_free_fluid(annotation):\n",
    "    video_path = annotation['video_path']\n",
    "    frame_number = int(annotation['frameNumber'])\n",
    "    foreground = annotation['data']['foreground']\n",
    "\n",
    "    print(f\"Processing video: {video_path}, frame: {frame_number}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Move to the annotated frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, old_frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Failed to read the frame number {frame_number} from the video.\")\n",
    "        return\n",
    "\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Combine all shapes into a single list of points\n",
    "    all_points = [point for shape in foreground for point in shape]\n",
    "    p0 = np.array(all_points, np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "    # print(f\"Initial points for tracking (p0): {p0}\")\n",
    "\n",
    "    mask = np.zeros_like(old_frame)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"No more frames to read or end of video reached.\")\n",
    "            break\n",
    "\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate optical flow\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "        if p1 is None or st is None:\n",
    "            print(\"Optical flow failed or no features found.\")\n",
    "            break\n",
    "\n",
    "        # Select good points\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "        errors = err[st == 1]\n",
    "\n",
    "        print(f\"New good points: {good_new}\")\n",
    "        print(f\"Old good points: {good_old}\")\n",
    "\n",
    "        if len(good_new) == 0 or len(good_old) == 0:\n",
    "            print(\"No good points found.\")\n",
    "            break\n",
    "\n",
    "        # Filter out points with high error\n",
    "        valid_new = []\n",
    "        valid_old = []\n",
    "        for new, old, e in zip(good_new, good_old, errors):\n",
    "            if e < confidence_threshold:\n",
    "                valid_new.append(new)\n",
    "                valid_old.append(old)\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "                frame = cv2.line(frame, (int(a), int(b)), (int(c), int(d)), (0, 255, 0), 2)\n",
    "                frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 255, 0), -1)\n",
    "\n",
    "        if len(valid_new) == 0 or len(valid_old) == 0:\n",
    "            print(\"No valid points found.\")\n",
    "            break\n",
    "\n",
    "        good_new = np.array(valid_new).reshape(-1, 1, 2)\n",
    "        good_old = np.array(valid_old).reshape(-1, 1, 2)\n",
    "\n",
    "        img = frame\n",
    "\n",
    "        cv2.imshow('frame', img)\n",
    "        if cv2.waitKey(30) & 0xFF == 27:\n",
    "            print(\"Stopped by user.\")\n",
    "            break\n",
    "\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "for _, annotation in random_annotations.iterrows():\n",
    "    display_annotation(annotation)\n",
    "    track_free_fluid(annotation)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
